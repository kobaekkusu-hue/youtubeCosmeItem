import typer
from typing import List, Optional
from sqlalchemy.orm import Session
from database import SessionLocal, engine, Base
from models import Product, Video, Review
from services.youtube import YouTubeService
from services.gemini import GeminiService
import logging
import re
import unicodedata
from difflib import SequenceMatcher
import json
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import time

# Build DB tables if they don't exist
Base.metadata.create_all(bind=engine)

app = typer.Typer()
logger = logging.getLogger(__name__)

# Amazonæ¤œç´¢ç”¨ãƒ˜ãƒƒãƒ€ãƒ¼
_SEARCH_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
}

def resolve_official_product_info(product_name: str, brand_name: str = None) -> dict:
    """
    Amazon ã®æ¤œç´¢çµæœã‹ã‚‰ç”»åƒURLãƒ»ä¾¡æ ¼ã‚’å–å¾—ã™ã‚‹ã€‚
    å•†å“åã¯Gemini ãŒæ¦‚è¦æ¬„ã‹ã‚‰æŠ½å‡ºã—ãŸæ­£å¼åç§°ã‚’ãã®ã¾ã¾ä½¿ç”¨ã™ã‚‹ã€‚
    
    Returns:
        dict: {'name': å•†å“å, 'image_url': ç”»åƒURL, 'price': ä¾¡æ ¼æ–‡å­—åˆ—}
    """
    result = {'name': product_name, 'image_url': '', 'price': None}
    
    # Amazon ã‹ã‚‰ç”»åƒãƒ»ä¾¡æ ¼ã‚’å–å¾—
    try:
        query = f"{brand_name} {product_name}" if brand_name else product_name
        amazon_url = f"https://www.amazon.co.jp/s?k={requests.utils.quote(query)}"
        resp = requests.get(amazon_url, headers=_SEARCH_HEADERS, timeout=10)
        
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'html.parser')
            first_result = soup.select_one('[data-component-type="s-search-result"]')
            
            if first_result:
                # ç”»åƒURLï¼ˆAmazonã¯å•†å“åã¯ä½¿ã‚ãšç”»åƒã ã‘å–å¾—ï¼‰
                img_el = first_result.select_one('img.s-image')
                if img_el:
                    src = img_el.get('src', '')
                    if src.startswith('http'):
                        result['image_url'] = src
                
                # ä¾¡æ ¼
                price_el = first_result.select_one('.a-price .a-offscreen')
                if price_el:
                    result['price'] = price_el.get_text(strip=True)
                    logger.info(f"  ä¾¡æ ¼: {result['price']}")
        
        time.sleep(1)
        
    except Exception as e:
        logger.warning(f"Amazonæ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({product_name}): {e}")
    
    return result

def normalize_name(name: str) -> str:
    """å•†å“åã‚’æ­£è¦åŒ–ã™ã‚‹ï¼ˆã‚¹ãƒšãƒ¼ã‚¹é™¤å»ã€å…¨è§’åŠè§’çµ±ä¸€ã€ã‚«ãƒƒã‚³å†…é™¤å»ï¼‰"""
    if not name:
        return ""
    # NFKCæ­£è¦åŒ–ï¼ˆå…¨è§’â†’åŠè§’ã€æ¿ç‚¹çµ±ä¸€ãªã©ï¼‰
    name = unicodedata.normalize('NFKC', name)
    # æ‹¬å¼§ã¨ãã®ä¸­èº«ã‚’é™¤å»ï¼ˆä¾‹: (Medicube) â†’ ç©ºï¼‰
    name = re.sub(r'[\(ï¼ˆ][^)ï¼‰]*[\)ï¼‰]', '', name)
    # ç©ºç™½ã‚’å…¨ã¦é™¤å»
    name = re.sub(r'\s+', '', name)
    # å°æ–‡å­—åŒ–ï¼ˆè‹±å­—ã®è¡¨è¨˜æºã‚Œå¯¾å¿œï¼‰
    name = name.lower()
    return name.strip()

def find_matching_product(db: Session, product_name: str, brand_name: str = None) -> Optional[Product]:
    """æ—¢å­˜ã®å•†å“ã‹ã‚‰åå¯„ã›ã§ä¸€è‡´ã™ã‚‹ã‚‚ã®ã‚’æ¢ã™"""
    normalized_new = normalize_name(product_name)
    if not normalized_new:
        return None
    
    # å…¨å•†å“ã‚’å–å¾—ã—ã¦æ¯”è¼ƒï¼ˆãƒ‡ãƒ¼ã‚¿é‡ãŒå°‘ãªã„ã†ã¡ã¯ã“ã‚Œã§ååˆ†ï¼‰
    all_products = db.query(Product).all()
    
    best_match = None
    best_score = 0.0
    
    for existing in all_products:
        normalized_existing = normalize_name(existing.name)
        
        # å®Œå…¨ä¸€è‡´ï¼ˆæ­£è¦åŒ–å¾Œï¼‰
        if normalized_new == normalized_existing:
            return existing
        
        # é¡ä¼¼åº¦è¨ˆç®—
        score = SequenceMatcher(None, normalized_new, normalized_existing).ratio()
        
        # ãƒ–ãƒ©ãƒ³ãƒ‰åã‚‚è€ƒæ…®ï¼ˆãƒ–ãƒ©ãƒ³ãƒ‰ãŒä¸€è‡´ã™ã‚Œã°é–¾å€¤ã‚’ä¸‹ã’ã‚‹ï¼‰
        if brand_name and existing.brand:
            brand_score = SequenceMatcher(
                None, 
                normalize_name(brand_name), 
                normalize_name(existing.brand)
            ).ratio()
            if brand_score > 0.7:
                score += 0.1  # ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹
        
        if score > best_score:
            best_score = score
            best_match = existing
    
    # é–¾å€¤0.85ä»¥ä¸Šãªã‚‰åŒä¸€å•†å“ã¨åˆ¤å®š
    if best_score >= 0.85 and best_match:
        logger.info(f"åå¯„ã›: '{product_name}' â†’ æ—¢å­˜ '{best_match.name}' (ã‚¹ã‚³ã‚¢: {best_score:.2f})")
        return best_match
    
    return None

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

@app.command()
def run_batch(query: str = "ã‚³ã‚¹ãƒ¡ ãƒ¬ãƒ“ãƒ¥ãƒ¼", max_videos: int = 5):
    """
    Search for videos, analyze them, and save results to the database.
    """
    db = SessionLocal()
    youtube_service = YouTubeService()
    gemini_service = GeminiService()

    logger.info(f"Starting batch process for query: {query}")

    # 1. Search Videos
    videos = youtube_service.search_videos(query, max_results=max_videos)
    logger.info(f"Found {len(videos)} videos.")

    for item in videos:
        video_id = item['id']['videoId']
        snippet = item['snippet']
        process_video_item(db, youtube_service, gemini_service, video_id, snippet)

    db.close()
    logger.info("Batch process completed.")

@app.command()
def process_urls(
    urls: List[str],
    api_key: str = typer.Option(None, help="Gemini APIã‚­ãƒ¼ (ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ç”¨)"),
    main_only: bool = typer.Option(False, help="å•†å“æŠ½å‡º(ãƒ¡ã‚¤ãƒ³å‡¦ç†)ã®ã¿å®Ÿè¡Œã—ã€ã‚¨ãƒ³ãƒªãƒƒãƒå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹")
):
    """
    Process specific YouTube videos by URL.
    Example: python batch_processor.py process-urls https://www.youtube.com/watch?v=... --api-key AIza... --main-only
    """
    from urllib.parse import urlparse, parse_qs

    db = SessionLocal()
    youtube_service = YouTubeService()
    
    # APIã‚­ãƒ¼ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
    keys = [api_key] if api_key else None
    gemini_service = GeminiService(api_keys=keys)

    for url in urls:
        # Extract Video ID
        parsed = urlparse(url)
        if parsed.hostname in ('youtu.be', 'www.youtu.be'):
            video_id = parsed.path[1:]
        elif parsed.hostname in ('youtube.com', 'www.youtube.com'):
            video_id = parse_qs(parsed.query).get('v', [None])[0]
        else:
            logger.warning(f"Invalid YouTube URL: {url}")
            continue

        if not video_id:
            logger.warning(f"Could not extract video ID from: {url}")
            continue

        logger.info(f"Processing URL: {url} -> ID: {video_id}")
        
        # Fetch Video Details
        video_details = youtube_service.get_video_details(video_id)
        if not video_details:
            logger.error(f"Could not fetch details for video {video_id}")
            continue
        
        snippet = video_details['snippet']
        process_video_item(
            db, 
            youtube_service, 
            gemini_service, 
            video_id, 
            snippet, 
            skip_enrich=main_only
        )

    db.close()
    logger.info("Custom video process completed.")

def process_video_item(db: Session, youtube_service: YouTubeService, gemini_service: GeminiService, video_id: str, snippet: dict, enrich_gemini_service: GeminiService = None, skip_enrich: bool = False):
    title = snippet['title']
    channel_name = snippet['channelTitle']
    description = snippet.get('description', '')  # æ¦‚è¦æ¬„ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    published_at_str = snippet['publishedAt']
    # Handle different date formats or ensure consistency. API usually returns ISO 8601
    try:
        published_at = datetime.fromisoformat(published_at_str.replace('Z', '+00:00'))
    except ValueError:
        # Fallback if format is different
        published_at = datetime.utcnow()

    thumbnail_url = snippet['thumbnails']['high']['url']

    # Check if video already exists
    existing_video = db.query(Video).filter(Video.id == video_id).first()
    if existing_video:
        logger.info(f"Video {video_id} already exists. Skipping.")
        return

    logger.info(f"Processing video: {title} ({video_id})")
    if description:
        logger.info(f"æ¦‚è¦æ¬„ã‚ã‚Š: {len(description)}æ–‡å­—")

    # 2. Get Transcript
    transcript = youtube_service.get_transcript(video_id)
    if not transcript:
        if description:
            # å­—å¹•ãªã—ã§ã‚‚æ¦‚è¦æ¬„ãŒã‚ã‚Œã°åˆ†æã‚’ç¶šè¡Œ
            logger.warning(f"å­—å¹•å–å¾—å¤±æ•— ({video_id})ã€‚æ¦‚è¦æ¬„ã®ã¿ã§å•†å“æŠ½å‡ºã‚’è©¦ã¿ã¾ã™ã€‚")
            transcript = [{"start": 0, "text": "ï¼ˆå­—å¹•ãªã—ï¼‰"}]
        else:
            logger.warning(f"No transcript and no description for video {video_id}. Skipping.")
            return

    # 3. Analyze with Geminiï¼ˆæ¦‚è¦æ¬„ + å­—å¹•ã‚’æ¸¡ã™ï¼‰
    logger.info(f"Analyzing video {video_id} with description + transcript...")
    try:
        analysis_results = gemini_service.analyze_video(
            transcript=transcript,
            description=description,
            title=title
        )
    except Exception as e:
        logger.error(f"Gemini analysis failed: {e}")
        return

    if not analysis_results:
        logger.info("No products found in video.")
        return

    # 4. Save to DB
    # Save Video
    new_video = Video(
        id=video_id,
        title=title,
        channel_name=channel_name,
        published_at=published_at,
        thumbnail_url=thumbnail_url
    )
    db.add(new_video)
    db.commit() # Commit video first to satisfy FK

    for result in analysis_results:
        product_name = result.get('product_name')
        if not product_name:
            continue

        # åå¯„ã›: æ­£è¦åŒ–åã¨é¡ä¼¼åº¦ã§æ—¢å­˜å•†å“ã‚’æ¤œç´¢
        brand_name = result.get('brand_name')
        product = find_matching_product(db, product_name, brand_name)
        if not product:
            # æ–°è¦å•†å“: Amazon ã‹ã‚‰æ­£è¦ã®å•†å“åãƒ»ç”»åƒãƒ»ä¾¡æ ¼ã‚’å–å¾—
            logger.info(f"æ–°è¦å•†å“æ¤œå‡º: '{product_name}' â†’ å…¬å¼æƒ…å ±ã‚’æ¤œç´¢ä¸­...")
            official_info = resolve_official_product_info(product_name, brand_name)
            
            product = Product(
                name=official_info['name'],
                brand=brand_name,
                category=result.get('category'),
                image_url=official_info['image_url'],
                price=official_info['price'],
            )
            db.add(product)
            db.commit()
            db.refresh(product)
            logger.info(f"æ–°è¦å•†å“ç™»éŒ²: '{official_info['name']}' (ID: {product.id[:8]}...)")
            
            # æ–°è¦å•†å“ã®è©³ç´°æƒ…å ±ã‚’Gemini AIã§ç”Ÿæˆ
            if not skip_enrich:
                enrich_svc = enrich_gemini_service or gemini_service
                enrich_new_product(product, enrich_svc, db)
                time.sleep(1)  # API ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            else:
                logger.info(f"  ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®ã¿å®Ÿè¡Œã®ãŸã‚ã‚¨ãƒ³ãƒªãƒƒãƒå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        # Save Review
        review = Review(
            product_id=product.id,
            video_id=video_id,
            timestamp_seconds=result.get('timestamp_seconds', 0),
            sentiment=result.get('sentiment', 'neutral'),
            summary=result.get('summary', '')
        )
        db.add(review)
    
    db.commit()
    count = db.query(Video).count()
    logger.info(f"Saved results for video {video_id}. Total videos in DB: {count}")


def enrich_new_product(product: Product, gemini_service: GeminiService, db):
    """æ–°è¦ç™»éŒ²ã—ãŸå•†å“ã®è©³ç´°æƒ…å ±ã‚’Gemini AIã§ç”Ÿæˆã™ã‚‹"""
    try:
        prompt = f"""ä»¥ä¸‹ã®ã‚³ã‚¹ãƒ¡å•†å“ã«ã¤ã„ã¦ã€æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

å•†å“å: {product.name}
ãƒ–ãƒ©ãƒ³ãƒ‰: {product.brand or 'ä¸æ˜'}
ã‚«ãƒ†ã‚´ãƒª: {product.category or 'ä¸æ˜'}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã€‚ç¢ºä¿¡ãŒãªã„æƒ…å ±ã¯ null ã«ã—ã¦ãã ã•ã„ã€‚å˜˜ã¯çµ¶å¯¾ã«å…¥ã‚Œãªã„ã“ã¨ã€‚

{{
  "description": "å•†å“ã®ç°¡æ½”ãªèª¬æ˜æ–‡ï¼ˆ100ã€œ200æ–‡å­—ç¨‹åº¦ï¼‰",
  "features": ["ç‰¹å¾´1", "ç‰¹å¾´2", "ç‰¹å¾´3"],
  "ingredients": "ä¸»ãªæˆåˆ†ï¼ˆã‚ã‹ã‚‹å ´åˆã®ã¿ï¼‰",
  "volume": "å®¹é‡ï¼ˆä¾‹: 30ml, 12gï¼‰",
  "how_to_use": "åŸºæœ¬çš„ãªä½¿ã„æ–¹ï¼ˆ50ã€œ100æ–‡å­—ç¨‹åº¦ï¼‰"
}}

JSONä»¥å¤–ã®æ–‡å­—ã‚’å«ã‚ãªã„ã“ã¨ã€‚"""

        response = gemini_service.model.generate_content(prompt)
        text = response.text.strip()
        if '```json' in text:
            text = text.split('```json')[1].split('```')[0].strip()
        elif '```' in text:
            text = text.split('```')[1].split('```')[0].strip()
        
        data = json.loads(text)
        
        if data.get('description'):
            product.description = data['description']
        if data.get('features') and isinstance(data['features'], list):
            product.features = json.dumps(data['features'], ensure_ascii=False)
        if data.get('ingredients'):
            product.ingredients = data['ingredients']
        if data.get('volume'):
            product.volume = data['volume']
        if data.get('how_to_use'):
            product.how_to_use = data['how_to_use']
        
        db.commit()
        logger.info(f"  å•†å“è©³ç´°ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
    except Exception as e:
        logger.warning(f"  å•†å“è©³ç´°ã®ç”Ÿæˆã«å¤±æ•—: {e}")


# ============================================================
# 3æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# ============================================================

# â‘  ã‚¿ã‚¤ãƒˆãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: ã€Œãƒ™ã‚¹ãƒˆã‚³ã‚¹ãƒ¡ã€ã€Œãƒ™ã‚¹ã‚³ã‚¹ã€ã‚’å«ã‚€å‹•ç”»ã®ã¿é€šã™
TITLE_PASS_KEYWORDS = [
    'ãƒ™ã‚¹ãƒˆã‚³ã‚¹ãƒ¡',
    'ãƒ™ã‚¹ã‚³ã‚¹',
]

def filter_by_title(title: str, description: str = '') -> bool:
    """
    â‘ ã‚¿ã‚¤ãƒˆãƒ«åˆ¤å®š: ã€Œãƒ™ã‚¹ãƒˆã‚³ã‚¹ãƒ¡ã€ã€Œãƒ™ã‚¹ã‚³ã‚¹ã€ã‚’å«ã‚€å‹•ç”»ã®ã¿é€šã™ã€‚
    
    Returns:
        True = é€šéï¼ˆå‡¦ç†å¯¾è±¡ï¼‰ã€False = ã‚¹ã‚­ãƒƒãƒ—
    """
    text = (title + ' ' + description).lower()
    # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã®ã„ãšã‚Œã‹ãŒã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯æ¦‚è¦æ¬„ã«å«ã¾ã‚Œã‚Œã°é€šã™
    for keyword in TITLE_PASS_KEYWORDS:
        if keyword.lower() in text:
            return True
    return False


# â‘¡ å­—å¹•å¯†åº¦åˆ¤å®šç”¨ã‚³ã‚¹ãƒ¡ç”¨èªè¾æ›¸
COSME_TERMS = [
    'ç™ºè‰²', 'ãƒ†ã‚¯ã‚¹ãƒãƒ£', 'ä¿æ¹¿', 'ä¹¾ç‡¥', 'ã‚¤ã‚¨ãƒ™', 'ãƒ–ãƒ«ãƒ™',
    'æ¯›ç©´', 'ã‚«ãƒãƒ¼åŠ›', 'å´©ã‚Œ', 'è‰²å‘³', 'ãƒ‘ã‚±', 'å††',
    'å¡—ã‚‹', 'ä»•ä¸ŠãŒã‚Š', 'ãƒ„ãƒ¤', 'ãƒãƒƒãƒˆ', 'ä¸‹åœ°', 'ãƒ©ãƒ¡',
    'ãƒ‘ã‚¦ãƒ€ãƒ¼', 'ãƒªã‚­ãƒƒãƒ‰', 'ãƒ•ã‚¡ãƒ³ãƒ‡', 'ãƒªãƒƒãƒ—', 'ã‚¢ã‚¤ã‚·ãƒ£ãƒ‰ã‚¦',
    'ãƒãƒ¼ã‚¯', 'ãƒã‚¹ã‚«ãƒ©', 'ã‚¢ã‚¤ãƒ©ã‚¤ãƒŠãƒ¼', 'ã‚³ãƒ³ã‚·ãƒ¼ãƒ©ãƒ¼',
    'ãƒ—ãƒ©ã‚¤ãƒãƒ¼', 'ãƒã‚¤ãƒ©ã‚¤ãƒˆ', 'ã‚·ã‚§ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°', 'ãƒ™ãƒ¼ã‚¹',
    'ã‚¹ã‚­ãƒ³ã‚±ã‚¢', 'åŒ–ç²§æ°´', 'ä¹³æ¶²', 'ç¾å®¹æ¶²', 'ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°',
    'æ—¥ç„¼ã‘æ­¢ã‚', 'SPF', 'UV', 'ãã™ã¿', 'ãƒˆãƒ¼ãƒ³ã‚¢ãƒƒãƒ—',
    'ãƒ•ã‚£ãƒƒãƒˆ', 'ãƒ¨ãƒ¬', 'ãƒ†ã‚«ãƒª', 'ã‚µãƒ©ã‚µãƒ©', 'ã—ã£ã¨ã‚Š',
    'ãƒŠãƒãƒ¥ãƒ©ãƒ«', 'é€æ˜æ„Ÿ', 'è¡€è‰²', 'ãƒ„ãƒ¤è‚Œ', 'ãƒãƒƒãƒˆè‚Œ',
    'ãƒ—ãƒãƒ—ãƒ©', 'ãƒ‡ãƒ‘ã‚³ã‚¹', 'ã‚³ã‚¹ãƒ¡', 'ãƒ¡ã‚¤ã‚¯',
]

def filter_by_transcript_density(transcript: list) -> float:
    """
    â‘¡å­—å¹•å¯†åº¦åˆ¤å®š: ã‚³ã‚¹ãƒ¡é–¢é€£ç”¨èªã®å‡ºç¾ç‡ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    
    Returns:
        float: ã‚³ã‚¹ãƒ¡ç”¨èªå¯†åº¦ï¼ˆ0.0ã€œ1.0ï¼‰ã€‚é«˜ã„ã»ã©ã‚³ã‚¹ãƒ¡é–¢é€£ã€‚
    """
    if not transcript:
        return 0.0
    
    # å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    full_text = ' '.join([item.get('text', '') for item in transcript])
    total_chars = len(full_text)
    
    if total_chars == 0:
        return 0.0
    
    # ã‚³ã‚¹ãƒ¡ç”¨èªã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    hit_count = 0
    for term in COSME_TERMS:
        hit_count += full_text.count(term)
    
    # å¯†åº¦ = ãƒ’ãƒƒãƒˆæ•° / ç·æ–‡å­—æ•° * 100ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆï¼‰
    density = (hit_count / total_chars) * 100
    return density


# å­—å¹•å¯†åº¦ã®é–¾å€¤ï¼ˆ%ï¼‰: ã“ã‚Œä»¥ä¸Šãªã‚‰ã‚³ã‚¹ãƒ¡é–¢é€£ã¨åˆ¤å®š
COSME_DENSITY_THRESHOLD = 0.3


def filter_by_ai_classification(
    gemini_service: GeminiService,
    title: str,
    description: str,
    transcript_sample: str
) -> bool:
    """
    â‘¢AIåˆ†é¡: Gemini Flash ã§ã€Œã‚³ã‚¹ãƒ¡ãƒ¬ãƒ“ãƒ¥ãƒ¼/ç´¹ä»‹ã‹ï¼Ÿã€ã‚’ Yes/No åˆ¤å®šã€‚
    
    Returns:
        True = ã‚³ã‚¹ãƒ¡ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨åˆ¤å®šã€False = ã‚³ã‚¹ãƒ¡ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§ã¯ãªã„
    """
    prompt = f"""ä»¥ä¸‹ã®YouTubeå‹•ç”»ã¯ã€Œã‚³ã‚¹ãƒ¡ï¼ˆåŒ–ç²§å“ï¼‰ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¾ãŸã¯ç´¹ä»‹å‹•ç”»ã€ã§ã™ã‹ï¼Ÿ
Yes ã‹ No ã®ã„ãšã‚Œã‹1å˜èªã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘
{title}

ã€æ¦‚è¦æ¬„ï¼ˆå†’é ­ï¼‰ã€‘
{description[:1000] if description else 'ï¼ˆãªã—ï¼‰'}

ã€å­—å¹•ï¼ˆå†’é ­ï¼‰ã€‘
{transcript_sample[:1500]}
"""
    try:
        response = gemini_service.model.generate_content(prompt)
        answer = response.text.strip().lower()
        is_cosme = answer.startswith('yes') or 'yes' in answer
        return is_cosme
    except Exception as e:
        logger.warning(f"AIåˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨å´ï¼ˆé€šã™ï¼‰
        return True


@app.command()
def process_channel(
    channel: str = typer.Argument(..., help="ãƒãƒ£ãƒ³ãƒãƒ«URLã€@ãƒãƒ³ãƒ‰ãƒ«ã€ã¾ãŸã¯ãƒãƒ£ãƒ³ãƒãƒ«ID"),
    max_videos: int = typer.Option(50, help="å–å¾—ã™ã‚‹æœ€å¤§å‹•ç”»æ•°"),
    density_threshold: float = typer.Option(COSME_DENSITY_THRESHOLD, help="å­—å¹•å¯†åº¦é–¾å€¤ï¼ˆ%ï¼‰"),
    skip_ai: bool = typer.Option(False, help="â‘¢AIåˆ†é¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹"),
    title_only: bool = typer.Option(False, help="â‘ ã‚¿ã‚¤ãƒˆãƒ«åˆ¤å®šã®ã¿ã§â‘¡â‘¢ã‚’ã‚¹ã‚­ãƒƒãƒ—"),
):
    """
    ç‰¹å®šYouTuberã®ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰å‹•ç”»ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§åé›†ã™ã‚‹ã€‚
    
    â‘  ã‚¿ã‚¤ãƒˆãƒ«ã«ã€Œãƒ™ã‚¹ãƒˆã‚³ã‚¹ãƒ¡ã€ã€Œãƒ™ã‚¹ã‚³ã‚¹ã€ã‚’å«ã‚€å‹•ç”»ã®ã¿é€šé
    â‘¡ å­—å¹•ã®ã‚³ã‚¹ãƒ¡ç”¨èªå¯†åº¦ãŒé–¾å€¤ä»¥ä¸Šã®å‹•ç”»ã®ã¿é€šéï¼ˆ--title-only ã§çœç•¥å¯ï¼‰
    â‘¢ Gemini AI ã§ã€Œã‚³ã‚¹ãƒ¡ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‹ï¼Ÿã€ã‚’ Yes/No åˆ¤å®šï¼ˆ--title-only ã§çœç•¥å¯ï¼‰
    
    Example:
        python batch_processor.py process-channel https://www.youtube.com/@cosmemory --title-only
        python batch_processor.py process-channel https://www.youtube.com/@cosmemory --max-videos 20
    """
    db = SessionLocal()
    youtube_service = YouTubeService()

    # ã‚­ãƒ¼ãƒ—ãƒ¼ãƒ«æ–¹å¼ã®å…±æœ‰GeminiServiceï¼ˆ10å€‹ã®ã‚­ãƒ¼ã‚’è‡ªå‹•ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
    gemini_service = GeminiService()

    # ãƒãƒ£ãƒ³ãƒãƒ«IDè§£æ±º
    logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ«ã‚’è§£æ±ºä¸­: {channel}")
    channel_id = youtube_service.resolve_channel_id(channel)
    if not channel_id:
        logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’è§£æ±ºã§ãã¾ã›ã‚“: {channel}")
        db.close()
        return
    logger.info(f"ãƒãƒ£ãƒ³ãƒãƒ«ID: {channel_id}")

    # ãƒãƒ£ãƒ³ãƒãƒ«ã®å‹•ç”»ä¸€è¦§ã‚’å–å¾—
    videos = youtube_service.get_channel_videos(channel_id, max_results=max_videos)
    if not videos:
        logger.error("å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        db.close()
        return

    logger.info(f"=== {len(videos)} æœ¬ã®å‹•ç”»ã‚’å–å¾—ã€‚3æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–‹å§‹ ===")

    stats = {'total': len(videos), 'pass_title': 0, 'pass_density': 0, 'pass_ai': 0, 'processed': 0, 'skipped_existing': 0}

    for i, video_info in enumerate(videos, 1):
        video_id = video_info['video_id']
        title = video_info['title']
        description = video_info['description']

        logger.info(f"\n[{i}/{len(videos)}] ğŸ“¹ {title}")

        # å‡¦ç†æ¸ˆã¿ãƒã‚§ãƒƒã‚¯
        existing = db.query(Video).filter(Video.id == video_id).first()
        if existing:
            logger.info(f"  â­ï¸  æ—¢ã«å‡¦ç†æ¸ˆã¿ã€‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            stats['skipped_existing'] += 1
            continue

        # ===== â‘  ã‚¿ã‚¤ãƒˆãƒ«åˆ¤å®š =====
        if not filter_by_title(title, description):
            logger.info(f"  âŒ â‘ ã‚¿ã‚¤ãƒˆãƒ«åˆ¤å®š: ã€Œãƒ™ã‚¹ãƒˆã‚³ã‚¹ãƒ¡/ãƒ™ã‚¹ã‚³ã‚¹ã€ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ â†’ ã‚¹ã‚­ãƒƒãƒ—")
            continue
        logger.info(f"  âœ… â‘ ã‚¿ã‚¤ãƒˆãƒ«åˆ¤å®š: é€šé")
        stats['pass_title'] += 1

        # ===== â‘¡ å­—å¹•å¯†åº¦åˆ¤å®š =====
        if not title_only:
            transcript = youtube_service.get_transcript(video_id)
            if not transcript:
                # å­—å¹•å–å¾—å¤±æ•—æ™‚ï¼šã‚¿ã‚¤ãƒˆãƒ«åˆ¤å®šã‚’é€šéã—ã¦ã„ã‚‹ã®ã§ã‚¹ã‚­ãƒƒãƒ—ã›ãšå…ˆã«é€²ã‚€
                logger.info(f"  âš ï¸  â‘¡å­—å¹•å–å¾—å¤±æ•— â†’ ã‚¿ã‚¤ãƒˆãƒ«åˆ¤å®šé€šéæ¸ˆã¿ã®ãŸã‚ã€å­—å¹•å¯†åº¦ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                stats['pass_density'] += 1
            else:
                density = filter_by_transcript_density(transcript)
                if density < density_threshold:
                    logger.info(f"  âŒ â‘¡å­—å¹•å¯†åº¦: {density:.2f}% < é–¾å€¤{density_threshold}% â†’ ã‚¹ã‚­ãƒƒãƒ—")
                    continue
                logger.info(f"  âœ… â‘¡å­—å¹•å¯†åº¦: {density:.2f}% â‰¥ é–¾å€¤{density_threshold}% â†’ é€šé")
                stats['pass_density'] += 1

            # ===== â‘¢ AIåˆ†é¡ =====
            if not skip_ai:
                transcript_sample = ''
                if transcript:
                    transcript_sample = ' '.join([item.get('text', '') for item in transcript[:50]])
                is_cosme = filter_by_ai_classification(gemini_service, title, description, transcript_sample)
                if not is_cosme:
                    logger.info(f"  âŒ â‘¢AIåˆ†é¡: ã‚³ã‚¹ãƒ¡ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§ã¯ãªã„ã¨åˆ¤å®š â†’ ã‚¹ã‚­ãƒƒãƒ—")
                    continue
                logger.info(f"  âœ… â‘¢AIåˆ†é¡: ã‚³ã‚¹ãƒ¡ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨åˆ¤å®š â†’ é€šé")
                time.sleep(1)  # API ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            stats['pass_ai'] += 1
        else:
            logger.info(f"  â© â‘¡â‘¢ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ--title-only ãƒ¢ãƒ¼ãƒ‰ï¼‰")
            stats['pass_density'] += 1
            stats['pass_ai'] += 1

        # ===== è©³ç´°æŠ½å‡º =====
        logger.info(f"  ğŸ” è©³ç´°æŠ½å‡ºé–‹å§‹...")
        snippet = {
            'title': title,
            'channelTitle': video_info['channel_name'],
            'description': description,
            'publishedAt': video_info['published_at'],
            'thumbnails': {'high': {'url': video_info['thumbnail_url']}},
        }
        process_video_item(db, youtube_service, gemini_service, video_id, snippet)
        stats['processed'] += 1

    # çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ
    logger.info(f"\n{'='*50}")
    logger.info(f"ğŸ“Š å‡¦ç†çµæœã‚µãƒãƒªãƒ¼")
    logger.info(f"{'='*50}")
    logger.info(f"  å…¨å‹•ç”»æ•°:        {stats['total']}")
    logger.info(f"  å‡¦ç†æ¸ˆã‚¹ã‚­ãƒƒãƒ—:  {stats['skipped_existing']}")
    logger.info(f"  â‘ ã‚¿ã‚¤ãƒˆãƒ«é€šé:   {stats['pass_title']}")
    logger.info(f"  â‘¡å­—å¹•å¯†åº¦é€šé:   {stats['pass_density']}")
    logger.info(f"  â‘¢AIåˆ†é¡é€šé:     {stats['pass_ai']}")
    logger.info(f"  è©³ç´°æŠ½å‡ºå®Œäº†:    {stats['processed']}")
    logger.info(f"{'='*50}")

    db.close()
    logger.info("ãƒãƒ£ãƒ³ãƒãƒ«å‡¦ç†å®Œäº†ã€‚")


if __name__ == "__main__":
    app()
